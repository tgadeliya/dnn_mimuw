{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise_4_Logistic_Regression_P3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "pJ-w7K4eu6SK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Logistic regression\n",
        "\n",
        "In this exercise you will train a logistic regression model via gradient descent in two simple scenarios.\n",
        " \n",
        "The general setup is as follows:\n",
        "* we are given a set of pairs $(x, y)$, where $x \\in R^D$ is a vector of real numbers representing the features, and $y \\in \\{0,1\\}$ is the target,\n",
        "* for a given $x$ we model the probability of $y=1$ by $h(x):=g(w^Tx)$, where $g$ is the sigmoid function: $g(z) = \\frac{1}{1+e^{-z}}$,\n",
        "* to find the right $w$ we will optimize the so called logarithmic loss: $J(w) = -\\frac{1}{n}\\sum_{i=1}^n y_i \\log{h(x_i)} + (1-y_i) \\log{(1-h(x_i))}$,\n",
        "* with the loss function in hand we can improve our guesses iteratively:\n",
        "    * $w_j^{t+1} = w_j^t - \\text{step_size} \\cdot \\frac{\\partial J(w)}{\\partial w_j}$,\n",
        "* we can end the process after some predefined number of epochs (or when the changes are no longer meaningful)."
      ]
    },
    {
      "metadata": {
        "id": "xt2z7CdJu6SQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's start with the simplest example - linear separated points on a plane. "
      ]
    },
    {
      "metadata": {
        "id": "Wg_d38Fou6SU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(123)\n",
        "\n",
        "# these parametrize the line\n",
        "a = 0.3\n",
        "b = -0.2\n",
        "c = 0.001\n",
        "\n",
        "# True/False mapping\n",
        "def lin_rule(x, noise=0.):\n",
        "    return a * x[0] + b * x[1] + c + noise < 0.\n",
        "\n",
        "# Just for plotting\n",
        "def get_y_fun(a, b, c):\n",
        "    def y(x):\n",
        "        return - x * a / b - c / b\n",
        "    return y\n",
        "\n",
        "lin_fun = get_y_fun(a, b, c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZZEHHKP8u6Si",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training data\n",
        "\n",
        "n = 500\n",
        "range_points = 1\n",
        "sigma = 0.05\n",
        "\n",
        "X = range_points * 2 * (np.random.rand(n, 2) - 0.5)\n",
        "y = [lin_rule(x, sigma * np.random.normal()) for x in X]\n",
        "\n",
        "print(X[:10])\n",
        "print(y[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CoTCKl3Yu6St",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's plot the data."
      ]
    },
    {
      "metadata": {
        "id": "qc99EecDu6Sw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "range_plot = 1.1\n",
        "h = .002\n",
        "\n",
        "plt.figure(figsize=(11,11))\n",
        "\n",
        "plt.scatter(X[:, 0], X[: , 1], c=y)\n",
        "\n",
        "_x = np.linspace(-range_plot, range_plot, 1000)\n",
        "_y = lin_fun(_x)\n",
        "\n",
        "plt.plot(_x, _y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vq3J7fZpu6S4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, let's implement and train a logistic regression model."
      ]
    },
    {
      "metadata": {
        "id": "Lw-eg0x0u6S6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################################################################\n",
        "# TODO: Implement logistic regression and compute its accuracy #\n",
        "################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BextVVMWu6TB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's visually asses our model. We can do this by using our estimates for $a,b,c$."
      ]
    },
    {
      "metadata": {
        "id": "odWHQD9Au6TE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(11,11))\n",
        "\n",
        "#################################################################\n",
        "# TODO: Pass your estimates for a,b,c to the get_y_fun function #\n",
        "#################################################################\n",
        "\n",
        "lin_fun2 = get_y_fun(...)\n",
        "\n",
        "_y2 = lin_fun2(_x)\n",
        "\n",
        "plt.figure(figsize=(11,11))\n",
        "plt.scatter(X[:, 0], X[: , 1], c=y)\n",
        "plt.plot(_x, _y, _x, _y2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u43DFWVFu6TO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now complicate the things a little bit and make our next problem nonlinear."
      ]
    },
    {
      "metadata": {
        "id": "qNCns_WIu6TS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Parameters of the ellipse\n",
        "s1 = 1.\n",
        "s2 = 2.\n",
        "r = 0.75\n",
        "m1 = 0.15\n",
        "m2 = 0.125\n",
        "\n",
        "# True/False mapping, checks whether we are inside the ellipse\n",
        "def circle_rule(x, noise=0.):\n",
        "    return s1 * (x[0] - m1) ** 2 + s2 * (x[1] - m2) ** 2 + noise < r ** 2."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H91RdYcOu6Tb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training data\n",
        "\n",
        "n = 500\n",
        "range_points = 1\n",
        "\n",
        "sigma = 0.1\n",
        "\n",
        "X = range_points * 2 * (np.random.rand(n, 2) - 0.5)\n",
        "\n",
        "y = [circle_rule(x, sigma * np.random.normal()) for x in X]\n",
        "\n",
        "print(X[:10])\n",
        "print(y[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1keKZp-su6Tl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's plot the data."
      ]
    },
    {
      "metadata": {
        "id": "_5qQnZLBu6Tr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "range_plot = 1.1\n",
        "h = .005\n",
        "\n",
        "plt.figure(figsize=(11,11))\n",
        "\n",
        "xx, yy = np.meshgrid(np.arange(-range_plot, range_plot, h), np.arange(-range_plot, range_plot, h))\n",
        "Z = np.array(list(map(circle_rule, np.c_[xx.ravel(), yy.ravel()])))\n",
        "\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "plt.scatter(X[:, 0], X[: , 1], c=y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qMwKzVQZu6Tw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, let's train a logistic regression model to tackle this problem. Note that we now need a nonlinear decision boundary. "
      ]
    },
    {
      "metadata": {
        "id": "Kcnc848fu6Tx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Hint: \n",
        "<sub><sup><sub><sup><sub><sup>\n",
        "Use feature engineering.\n",
        "</sup></sub></sup></sub></sup></sub>"
      ]
    },
    {
      "metadata": {
        "id": "cPINtZzou6T0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################################################################\n",
        "# TODO: Implement logistic regression and compute its accuracy #\n",
        "################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8nYLJvI4u6T7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's visually asses our model. \n",
        "\n",
        "Contrary to the previous scenario, converting our weights to parameters of the ground truth curve may not be straightforward. It's easier to just provide predictions for a set of points in $R^2$."
      ]
    },
    {
      "metadata": {
        "id": "8vn13Nfuu6T9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "range_plot = 1.1\n",
        "h = .005\n",
        "\n",
        "xx, yy = np.meshgrid(np.arange(-range_plot, range_plot, h), np.arange(-range_plot, range_plot, h))\n",
        "X_plot = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "print(X_plot)\n",
        "print(X_plot.shape)\n",
        "\n",
        "############################################################\n",
        "# TODO: Compute true/false predictions for the X_plot data #\n",
        "############################################################\n",
        "\n",
        "preds = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cE_jWcRZu6UG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(11,11))\n",
        "\n",
        "Z = preds\n",
        "Z = np.array(Z).reshape(xx.shape)\n",
        "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "Z = np.array(list(map(circle_rule, X_plot)))\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.pcolormesh(xx, yy, Z, alpha=0.1, cmap=plt.cm.Paired)\n",
        "\n",
        "plt.scatter(X[:, 0], X[: , 1], c=y)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}